{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4b53088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy: 85.98%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "mapping = {}\n",
    "with open(\"dataset/emnist-byclass-mapping.txt\") as f:\n",
    "    for line in f:\n",
    "        label, ascii_code = line.strip().split()\n",
    "        mapping[int(label)] = chr(int(ascii_code))\n",
    "\n",
    "def label_to_char(label):\n",
    "    return mapping.get(label, None)\n",
    "\n",
    "# === Model Definition (same as training) ===\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 5 * 5, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Device config (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Load Test Data for batch inference ===\n",
    "test_data = pd.read_csv(\"dataset/emnist-byclass-test.csv\", header=None)\n",
    "true_labels = test_data.iloc[:, 0].values  # True labels for checking accuracy\n",
    "images = test_data.iloc[:, 1:].values.reshape(-1,28,28,1).astype('float32') / 255.0\n",
    "images_torch = torch.from_numpy(images).permute(0,3,1,2)\n",
    "\n",
    "# Test transforms (for batch and single images)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# === Custom Dataset for Test Images ===\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class EMNISTTestDataset(Dataset):\n",
    "    def __init__(self, images, transform=None):\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, idx):\n",
    "        img_tensor = self.images[idx]\n",
    "        img_np = img_tensor.numpy()\n",
    "        if img_np.ndim == 3 and img_np.shape[0] == 1:\n",
    "            img_np = img_np.squeeze(0)\n",
    "        elif img_np.ndim == 3 and img_np.shape[-1] == 1:\n",
    "            pass\n",
    "        elif img_np.ndim != 2:\n",
    "            raise ValueError(f\"Unexpected image shape: {img_np.shape}\")\n",
    "        if self.transform:\n",
    "            img_out = self.transform(img_np)\n",
    "        else:\n",
    "            img_out = torch.tensor(img_np)\n",
    "        return img_out\n",
    "\n",
    "batch_size = 32\n",
    "test_dataset = EMNISTTestDataset(images_torch, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# === Load Trained Model ===\n",
    "num_classes = 62\n",
    "model = CNN(num_classes=num_classes).to(device)\n",
    "model.load_state_dict(torch.load(\"cnn_model.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# === Batch Inference ===\n",
    "all_predicted_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch_x in test_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_predicted_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "predicted_chars = [label_to_char(lbl) for lbl in all_predicted_labels]\n",
    "\n",
    "# === Check accuracy and print summary ===\n",
    "correct = sum(p == t for p, t in zip(all_predicted_labels, true_labels))\n",
    "total = len(true_labels)\n",
    "accuracy = (correct / total) * 100\n",
    "print(f\"Prediction Accuracy: {accuracy:.2f}%\\n\")\n",
    "\n",
    "# === Save batch predictions to CSV ===\n",
    "output_df = pd.DataFrame({\"Predicted\": predicted_chars})\n",
    "output_df.to_csv(\"emnist_predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98fd4fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def predict_character_from_flat(flat_array):\n",
    "    \"\"\"\n",
    "    Predict an EMNIST character from a flat 784-length array (pixels) using a trained model.\n",
    "\n",
    "    Args:\n",
    "        flat_array (list or np.ndarray): Length-784 pixel values (0-255) for a single 28x28 image.\n",
    "\n",
    "    Returns:\n",
    "        str: Predicted character.\n",
    "    \"\"\"\n",
    "    # Step 1: Convert flat array to numpy and reshape to 28x28\n",
    "    img_np = np.array(flat_array, dtype=np.uint8).reshape(28, 28)\n",
    "    \n",
    "    # Step 2: Convert to PIL Image (grayscale)\n",
    "    pil_img = Image.fromarray(img_np)\n",
    "\n",
    "    # Step 4: Resize (redundant here but safe for general use)\n",
    "    pil_img = pil_img.resize((28, 28), Image.Resampling.LANCZOS)\n",
    "    \n",
    "    # Step 5: Convert to tensor, normalize to [0,1]\n",
    "    img_tensor = transforms.ToTensor()(pil_img).unsqueeze(0).to(device) # Add batch dimension\n",
    "    \n",
    "    # Step 6: Model inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        _, predicted_label = torch.max(output.data, 1)\n",
    "        predicted_label = predicted_label.item()\n",
    "    \n",
    "    # Step 7: Decode to character\n",
    "    char = label_to_char(predicted_label)\n",
    "    return char\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "072bacb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_flat_image(flat_array, cmap='gray'):\n",
    "    \"\"\"\n",
    "    Display a 784-length flat pixel array as a 28x28 image.\n",
    "    Args:\n",
    "        flat_array (list or np.ndarray): Length-784 pixel values (0-255 or normalized 0-1).\n",
    "        cmap (str): Matplotlib color map ('gray' by default).\n",
    "    \"\"\"\n",
    "    if len(flat_array) != 784:\n",
    "        raise ValueError(\"Input array must be of length 784\")\n",
    "    img = np.array(flat_array).reshape(28, 28)\n",
    "    img = np.flipud(np.rot90(img))\n",
    "    plt.imshow(img, cmap=cmap)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# test_flat = [13,0,0,0,...]  # Your full array here!\n",
    "# show_flat_image(test_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aac58724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(img, real_Val):\n",
    "    show_flat_image(img)\n",
    "    print(f\"Real value : \", label_to_char(real_Val))\n",
    "    print(predict_character_from_flat(img))\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e487597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_random_predictions(csv_path, num_samples=5):\n",
    "    \"\"\"\n",
    "    Picks num_samples random rows from csv, calls pred() for each image.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(csv_path, header=None)\n",
    "    chosen_indices = np.random.choice(len(data), size=num_samples, replace=False)\n",
    "    for idx in chosen_indices:\n",
    "        row = data.iloc[idx].values\n",
    "        true_label = row[0]\n",
    "        img_flat = row[1:]\n",
    "        pred(img_flat, true_label)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2034f65a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real value :  7\n",
      "7\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real value :  b\n",
      "b\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real value :  8\n",
      "8\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real value :  x\n",
      "x\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real value :  8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "test_random_predictions(\"dataset/emnist-byclass-test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa0e321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def preprocess_to_emnist(image_path):\n",
    "    # Load image and convert to grayscale\n",
    "    img = Image.open(image_path).convert('L')\n",
    "\n",
    "    # Invert colors (if background is white)\n",
    "    img = ImageOps.invert(img)\n",
    "\n",
    "    # Resize to 28x28\n",
    "    img = img.resize((28, 28), Image.Resampling.LANCZOS)\n",
    "\n",
    "    # Convert to numpy array\n",
    "    img_np = np.array(img)\n",
    "\n",
    "    # Optional: Center the digit by its moments (like EMNIST style)\n",
    "    # Compute the center of mass\n",
    "    moments = cv2.moments(img_np)\n",
    "    if moments[\"m00\"] != 0:\n",
    "        cx = int(moments[\"m10\"] / moments[\"m00\"])\n",
    "        cy = int(moments[\"m01\"] / moments[\"m00\"])\n",
    "        shiftx = np.round(14 - cx).astype(int)\n",
    "        shifty = np.round(14 - cy).astype(int)\n",
    "        M = np.float32([[1, 0, shiftx], [0, 1, shifty]])\n",
    "        img_np = cv2.warpAffine(img_np, M, (28, 28))\n",
    "\n",
    "    # Normalize to [0,1]\n",
    "    img_np = img_np.astype(np.float32) / 255.0\n",
    "\n",
    "    return img_np\n",
    "\n",
    "def predict_character(image):\n",
    "    \"\"\"\n",
    "    Predict the character from a single handwritten image,\n",
    "    accepts either a file path or a preprocessed numpy array.\n",
    "    \n",
    "    Args:\n",
    "        image (str or np.ndarray or PIL.Image): Image file path OR processed numpy array OR PIL image.\n",
    "    \n",
    "    Returns:\n",
    "        str: Predicted character.\n",
    "    \"\"\"\n",
    "    if isinstance(image, str):\n",
    "        # If a file path, open and preprocess\n",
    "        pil_img = Image.open(image).convert(\"L\")\n",
    "        pil_img = ImageOps.invert(pil_img)\n",
    "        pil_img = pil_img.resize((28, 28), Image.Resampling.LANCZOS)\n",
    "        img_tensor = transforms.ToTensor()(pil_img).unsqueeze(0).to(device)\n",
    "    elif isinstance(image, np.ndarray):\n",
    "        # If numpy array already preprocessed (e.g., 28x28 float)\n",
    "        # Ensure it's in the right shape and type for ToTensor\n",
    "        if image.ndim == 2:\n",
    "            # Convert numpy [0,1] float32 grayscale to PIL Image for consistent transform\n",
    "            pil_img = Image.fromarray((image * 255).astype(np.uint8))\n",
    "            img_tensor = transforms.ToTensor()(pil_img).unsqueeze(0).to(device)\n",
    "        else:\n",
    "            raise ValueError(\"NumPy array should be 2D grayscale image\")\n",
    "    elif isinstance(image, Image.Image):\n",
    "        pil_img = image.convert(\"L\")\n",
    "        img_tensor = transforms.ToTensor()(pil_img).unsqueeze(0).to(device)\n",
    "    else:\n",
    "        raise TypeError(\"Input must be file path (str), numpy array, or PIL Image\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        _, predicted_label = torch.max(output.data, 1)\n",
    "        predicted_label = predicted_label.item()\n",
    "\n",
    "    return label_to_char(predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17967efe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
